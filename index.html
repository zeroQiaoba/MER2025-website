<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="MER25 Challenge and MRAC25 Workshop">
    <meta property="og:image" content="./img/mer2023.jpg">
    <meta property="og:type" content="website">
    
    <title>
        MER25 Challenge and MRAC25 Workshop
    </title>

    <!-- Bootstrap Core CSS -->
    <link href="./css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="./css/agency.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="./css/font-awesome.min.css" rel="stylesheet" >
    <link href="./css/style.css" rel="stylesheet">
    <!-- <link href="./css/css(1)" rel="stylesheet" type="text/css">
    <link href="./css/css(2)" rel="stylesheet" type="text/css">
    <link href="./css/css(3)" rel="stylesheet" type="text/css"> -->

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script type="text/javascript" async="" src="./js/analytics.js"></script>
    <script async="" src="./js/js"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', 'UA-114495477-1');
    </script>


</head>

<body id="page-top" class="index">

    <!-- Navigation -->
    <nav class="navbar navbar-default navbar-fixed-top">
        <div class="container">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>

            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                    <li class="hidden active">
                        <a href="https://MER2025.github.io/"></a>
                    </li>
                    <li class="">
                        <a class="page-scroll" href="#introduction">Intro</a>
                    </li>
					<li>
                        <a class="page-scroll" href="#news">News</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#challenge">Challenge</a>
                    </li>
					<li>
                        <a class="page-scroll" href="#workshop">Workshop</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#schedule">Schedule</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#speakers">Speakers</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#organization">Organizers</a>
                    </li>
                    <!-- <li>
                        <a class="page-scroll" href="#program_committee">Program Committee</a>
                    </li> -->

                   
                </ul>
            </div>
        </div>
    </nav> 

    <!-- Header -->
    <header_3>
        <div class="container">
            <div class="intro-text" style="color:white;">
                <br>        
                <div class="intro-heading">MER25@ACM MM and MRAC25@ACM MM</div>
                <br><br>              
                <div class="intro-heading-large">Affective Computing</div>
                <br><br><br>
                <div class="intro-heading-large">Meets Large</div>
                <br><br><br>
                <div class="intro-heading-large">Language Models</div>
                <br><br>
            </div>
        </div>
    </header_3>

    <!-- Introduction Section -->
    <section id="introduction" style="padding-top: 30px;">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                </div>
            </div>
            <div class="row text-justify">
                <div class="col-md-12" >

                    <p class="large text-muted ">
                
                        <br>
                        
                        <b>MER2025</b> is the third year of our MER series of challenges, aiming to bring 
                        together researchers in the affective computing community to explore emerging trends and future directions in the field. 
                        Previously, <a href="http://merchallenge.cn/mer2023">MER23 Challenge@ACM MM</a> focused on multi-label learning, noise robustness, and semi-supervised learning, 
                        while <a href="https://zeroqiaoba.github.io/MER2024-website/">MER24 Challenge@IJCAI</a> introduced a new track dedicated to open-vocabulary emotion recognition. 
                        This year, MER2025 centers on the theme "Affective Computing Meets Large Language Models". 
                        We aim to shift the paradigm from traditional categorical frameworks reliant on predefined emotion taxonomies 
                        to large language model (LLM)-driven generative methods, offering innovative solutions for more accurate and reliable emotion understanding. 
                        The challenge features four tracks: 
                        <b>MER-SEMI</b> focuses on fixed categorical emotion recognition enhanced by semi-supervised learning; 
                        <b>MER-FG</b> explores fine-grained emotions, expanding recognition from basic to nuanced emotional states; 
                        <b>MER-DES</b> incorporates multimodal cues (beyond emotion words) into predictions to enhance model interpretability; 
                        <b>MER-PR</b> investigates whether emotion prediction results can improve personality recognition performance. 
                        For the first three tracks, baseline code is available at <a href="https://github.com/zeroQiaoba/MERTools/tree/master/MER2025">MERTools</a> and 
                        datasets can be accessed via <a href="https://huggingface.co/datasets/MERChallenge/MER2025">Hugging Face.</a>
                        For the last track, the dataset and baseline code are available on 
                        <a href="https://github.com/cai-cong/MER25_personality">GitHub.</a>

                    </p>

                </div>
            </div>
        </div>
    </section>


<!-- Call for Papers Section -->
<section id="news" class="bg-mid-gray">
    <div class="container">
        <div class="row">
            <div class="col-lg-12 text-center">
                <h2 class="section-heading">News</h2>
                <!-- <h3 class="section-subheading text-muted" style="font-size:large">Contact with us: <a href="mailto:fl4pwsdm@gmail.com">fl4pwsdm@gmail.com</a></h3> -->
            </div>
        </div>
        <div class="text-left" style="text-align: center;">
            <div class="col-md-12">
                <p class="large text-muted">
                <span> April 30, 2025: </span><b>We establish an initial website for MER25 Challenge and MRAC25 Workshop</b>
                </p>
            </div>

            <div class="col-md-12">
                <p class="large text-muted">
                <span> June 23, 2025: </span><b>We provide CodaLab links for Track1 ~ Track4. For all tracks, the final submissions should be in English.</b>
                </p>
            </div>

            <div class="col-md-12">
                <p class="large text-muted">
                <span> Jul 10, 2025: </span><b>Due to the recent CodaLab crash, we extend the Result Submission Deadline to July 13th.</b>
                </p>
            </div>

            <div class="col-md-12">
                <p class="large text-muted">
                <span> Jul 18, 2025: </span><b>We provide the paper submission link and extend the Paper Submission Deadline to July 22th.</b>
                </p>
            </div>
            
            <!-- <div class="col-md-12">
                <p class="large text-muted">
                <span> May 7, 2025: </span><b>For MER-OV, we require that participants cannot use closed-source models (such as GPT or Claude). </b>
                </p>
            </div>
            <div class="col-md-12">
                <p class="large text-muted">
                <span> May 26, 2025: </span><b>For MER-OV, we update the <a href="https://github.com/zeroQiaoba/MERTools/tree/master/MER2024">evaluation code </a>, <a href="https://arxiv.org/abs/2404.17113"> baseline paper </a> and <a href="https://arxiv.org/pdf/2306.15401"> EMER</a>.</b>
                </p>
            </div>
            <div class="col-md-12">
                <p class="large text-muted">
                <span> June 18, 2025: </span><b></b>
                </p>
            </div>
            <div class="col-md-12">
                <p class="large text-muted">
                <span> June 18, 2025: </span><b>For MER-OV, final-openset-*.csv in the provided dataset is the labels extracted from EMER descriptions. <a href="https://github.com/zeroQiaoba/MERTools/blob/master/MER2024/ov_store/check-openset.csv"> check-openset.csv </a> in github is the final checked ground truths. </b>
                </p>
            </div>
            
            <div class="col-md-12">
                <p class="large text-muted">
                <span> July 10, 2025: </span><b>The competition results submission deadline has now passed. We would like to express our sincere gratitude for your support during MER24. We encourage all participants to submit papers to MRAC24@ACM Multimedia. </b>
                </p>
            </div>

            <div class="col-md-12">
                <p class="large text-muted">
                <span> Sep 17, 2025: </span><b> All accepted papers are listed on this website. </b>
                </p>
            </div>

            <div class="col-md-12">
                <p class="large text-muted">
                <span> Oct 10, 2025: </span><b> We release the workshop schedule. Hope to see you in Melbourne. </b>
                </p>
            </div>

            <div class="col-md-12">
                <p class="large text-muted">
                <span> Nov 5, 2025: </span><b> We share the <a href="https://pan.baidu.com/s/1VhM69CLjnxEDTQchyRbzQA?pwd=bqfc"> Recorded video </a> for our workshop.</b>
                </p>
            </div> -->

        </div>
    </div>
</section>	
	
	
<!-- Call for Papers Section -->
<section id="challenge">
    <div class="container">
        <div class="row">
            <div class="col-lg-12 text-center">
                <h2 class="section-heading">MER25 Challenge@ACM MM</h2>
                <!-- <h3 class="section-subheading text-muted" style="font-size:large">Contact with us: <a href="mailto:fl4pwsdm@gmail.com">fl4pwsdm@gmail.com</a></h3> -->
            </div>
        </div>
        <div class="row text-justify">
            <div class="col-md-12">

                <br>
				<p class="large text-muted">
                    <b>Track 1. MER-SEMI.</b> MER-SEMI spans three consecutive MER challenges, 
                    aiming to enhance the performance of categorical emotion recognition algorithms through semi-supervised learning and unlabeled data. 
                    This year, we expanded the dataset by incorporating more labeled and unlabeled samples.
                    Participants are encouraged to leverage semi-supervised learning techniques, such as masked auto-encoders or contrastive learning, to achieve better results.
                </p>

                <p class="large text-muted">
                    <b>Track 2. MER-FG.</b> Current frameworks primarily focus on basic emotions,
                     often failing to capture the complexity and subtlety of human emotions. 
                     This track shifts the emphasis to fine-grained MER, enabling the prediction of a broader range of emotions. 
                     Following previous works [1, 2], 
                     participants are encouraged to leverage LLMs for this purpose. 
                     Given that LLMs possess extensive vocabularies, they hold the potential to generate more diverse emotion categories beyond basic labels.
                </p>


                <p class="large text-muted">
                    <b>Track 3. MER-DES.</b> The first two tracks primarily focus on emotion words, 
                    neglecting the integration of multimodal clues during the inference process. 
                    This omission results in prediction outcomes that lack interpretability. 
                    Moreover, emotion words struggle to fully capture the dynamic, diverse, and sometimes ambiguous nature of human emotions. 
                    This track seeks to leverage free-form, natural language descriptions to represent emotions [2, 3], 
                    offering greater flexibility to achieve more accurate emotion representations and enhance model interpretability.
                </p>


                <p class="large text-muted">
                    <b>Track 4. MER-PR.</b> Personality and emotion are deeply intertwined in human behavior and social interactions, 
                    yet current research often treats them as separate tasks, neglecting their inherent correlations. 
                    This track seeks to investigate the interplay between emotion and personality, 
                    exploring whether emotion recognition can enhance the accuracy of personality predictions. 
                    Participants are encouraged to employ techniques such as multi-task learning to analyze the influence of emotion on personality prediction.
                </p>

                <br>



                <!-- <head>
                    <title>MathJax Example</title>
                    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
                    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
                </head> -->
                <!-- <p class="large text-muted">
                    <b>Evaluation Metrics.</b> For MER-SEMI and MER-NOISE, 
                    we choose two widely used metrics in emotion recognition: accuracy and weighted average F-score (WAF). 
                    Considering the inherent class imbalance, we choose WAF as the final ranking.
                    For MER-OV, we draw on our previous work [4], in which we extend traditional classification metrics (i.e., accuracy and recall) and define set-level accuracy and recall.
                    More details can be found in our baseline code. 
    
                </p> -->
                
                <!-- <br> -->
                <p class="large text-muted">
                    <b>Dataset.</b> 
                    For the first three tracks, to download the dataset, participants must complete an EULA available on 
                    <a href="https://huggingface.co/datasets/MERChallenge/MER2025">Hugging Face</a>. 
                    The EULA clearly states that the dataset is for academic research purposes only and prohibits any modifications or uploads to the Internet. 
                    We will review the submitted EULA promptly; once approved, participants will gain access to the dataset.

                    For the last track, the dataset and baseline code are available on <a href="https://github.com/cai-cong/MER25_personality">GitHub.</a>
                </p>
                
                <br>
                <p class="large text-muted">
                    <b>Result submission.</b> 
                    For <b>MER-SEMI, MER-FG, and MER-DES</b>, the test samples are selected from the 124k unlabeled samples. 
                    To reduce the task difficulty, we reduce the evaluation scope from 124k to <a href="https://huggingface.co/datasets/MERChallenge/MER2025"> 20k candidate samples </a>. 
                    Participants must submit predictions for these 20k samples, which cover all the test samples of these tracks. 
                    Each track has distinct objectives. 
                    For MER-SEMI, participants must predict the most likely label from six predefined categories: <i> worried, happy, neutral, angry, surprise, and sad </i>; 
                    For MER-FG, participants can freely predict any emotion labels without restrictions on category or quantity; 
                    For MER-DES, participants are required to submit both multimodal evidence and corresponding emotion labels to improve model interpretability.
                    For <b>MER-PR</b>, we provide an official test set, and participants can directly submit predictions for the test set.
                </p>
                
                <p style="color: rgb(135,206,235)">CodaLab link for MER2025-SEMI: <a href="https://codalab.lisn.upsaclay.fr/competitions/23154"> https://codalab.lisn.upsaclay.fr/competitions/23154  </a> </p>
                <p style="color: rgb(135,206,235)">CodaLab link for MER2025-FG:   <a href="https://codalab.lisn.upsaclay.fr/competitions/23155"> https://codalab.lisn.upsaclay.fr/competitions/23155  </a> </p>
                <p style="color: rgb(135,206,235)">CodaLab link for MER2025-DES:  <a href="https://codalab.lisn.upsaclay.fr/competitions/23156"> https://codalab.lisn.upsaclay.fr/competitions/23156  </a> </p>
                <p style="color: rgb(135,206,235)">CodaLab link for MER2025-PR:   <a href="https://codalab.lisn.upsaclay.fr/competitions/23185"> https://codalab.lisn.upsaclay.fr/competitions/23185  </a> </p>
                
                <!-- <p class="large text-muted">
                    <b> Note: </b> please register on Codalab using the email provided on the EULA or the email where you sent the EULA. 
                    The rankings of MER2024-SEMI and MER2024-NOISE are based on the CodaLab leaderboard. 
                    But for MER2024-OV, CodaLab is only used for format check. 
                    Each team can submit five times to the official email, the best performance among them used for final ranking​​.
                    Due to slightly randomness of GPT-3.5, we will run the evaluation code five times and report the average score. 
                    The ranking of MER2024-OV track will be announced a few weeks after the competition ends.
                </p> -->

                <br>
                <p class="large text-muted">
                    <b>Paper submission.</b> All participants are encouraged to submit a paper describing their solution to MRAC25 Workshop.
                    <!-- Top-5 teams in each track <b>MUST</b> submit a paper.
                    Top-3 winning teams in each track will be awarded with a certificate. -->
                    Paper sumbission link: <a href="https://cmt3.research.microsoft.com/MER2025"> https://cmt3.research.microsoft.com/MER2025 </a>
                </p>
                
                <br>
                <p class="large text-muted">
                    <b>Baseline paper:</b> <a href="https://arxiv.org/abs/2504.19423"> https://arxiv.org/abs/2504.19423 </a>
                    <br>
                    <b>Baseline code (Track1~Track3):</b> <a href="https://github.com/zeroQiaoba/MERTools/tree/master/MER2025">https://github.com/zeroQiaoba/MERTools/tree/master/MER2025 </a>
                    <br>
                    <b>Baseline code (Track4):</b> <a href="https://github.com/cai-cong/MER25_personality">https://github.com/cai-cong/MER25_personality </a>
                    <br>
                    <b>Contact email:</b> <a href="merchallenge.contact@gmail.com">merchallenge.contact@gmail.com;</a> <a href="lianzheng2016@ia.ac.cn">lianzheng2016@ia.ac.cn</a>
                </p>

                <br>

                <br>
                <p class="text-muted"> 
					[1] Zheng Lian, Haiyang Sun, Licai Sun, Haoyu Chen, Lan Chen, Hao Gu, Zhuofan Wen, Shun Chen, Siyuan Zhang, Hailiang Yao, Bin Liu, Rui Liu, Shan Liang, Ya Li, Jiangyan Yi, Jianhua Tao. 
                    <a href="https://arxiv.org/abs/2410.01495">OV-MER: Towards Open-Vocabulary Multimodal Emotion Recognition. </a>
                    arXiv preprint
                    arXiv:2410.01495, 2024.
					<br>
                    [2] Zheng Lian, Haoyu Chen, Lan Chen, Haiyang Sun, Licai Sun, Yong Ren, Zebang Cheng, Bin Liu, Rui Liu, Xiaojiang Peng, Jiangyan Yi, Jianhua Tao. 
                    <a href="https://arxiv.org/abs/2501.16566">AffectGPT: A New Dataset, Model, and Benchmark for Emotion Understanding with Multimodal Large Language Models. </a>
                    arXiv preprint
                    arXiv:2501.16566, 2025.
					<br>
                    [3] Zheng Lian, Haiyang Sun, Licai Sun, Hao Gu, Zhuofan Wen, Siyuan Zhang, Shun Chen, Mingyu Xu, Ke Xu, Kang Chen, Lan Chen, Shan Liang, Ya Li, Jiangyan Yi, Bin Liu, Jianhua Tao. 
                    <a href="https://arxiv.org/abs/2306.15401v5">Explainable Multimodal Emotion Recognition. </a>
                    arXiv preprint
                    arXiv:2306.15401v5, 2024.
					<br>
                </p>
            </div>
        </div>
    </div>
</section>


<section id="workshop" class="bg-mid-gray">
    <div class="container">
        <div class="row">
            <div class="col-lg-12 text-center">
                <h2 class="section-heading">MRAC25 Workshop@ACM MM</h2>
                <!-- <h3 class="section-subheading text-muted" style="font-size:large">Contact with us: <a href="mailto:fl4pwsdm@gmail.com">fl4pwsdm@gmail.com</a></h3> -->
            </div>
        </div>
        <div class="row text-justify">
            <div class="col-md-12" >

                <p class="large text-muted ">
                    <br><br>Besides papers for the MER25 Challenge, we also invite submissions on any aspect of multimodal emotion recognition and synthesis in deep learning. 
                    <b>Topics include but not limited to:</b> </br>
                </p>


                <ul class="large text-muted">

                    
                    <td style="width: 45%; vertical-align:top"><li> Large scale data generation or Inexpensive annotation for Affective Computing</li></td>
                    <td style="width: 45%; vertical-align:top"><li> Generative AI for Affective Computing using multimodal signals</li></td>
                    <td style="width: 45%; vertical-align:top"><li> Multi-modal method for emotion recognition </li></td>
                    <td style="width: 45%; vertical-align:top"><li> Privacy preserving large scale emotion recognition in the wild </li></td>
                    <td style="width: 45%; vertical-align:top"><li> Affective Computing Applications in education, entertainment & healthcare </li></td>
                    <td style="width: 45%; vertical-align:top"><li> Explainable or Privacy Preserving AI in affective computing</li></td>
                    <td style="width: 45%; vertical-align:top"><li> Generative and responsible personalization of affective phenomena estimators with few-shot learning</li></td>
                    <td style="width: 45%; vertical-align:top"><li> Bias in affective computing data (e.g. lack of multi-cultural datasets)</li></td>
                    <td style="width: 45%; vertical-align:top"><li>Semi-/weak-/un-/self- supervised learning methods and other novel methods for Affective Computing</li></td>
                    
                </ul>

                <p class="large text-muted ">
                    <br>
                    <b>Format:</b> Submitted papers (.pdf format) must use the ACM Article Template: <a href="https://acmmm2025.org/call-for-papers/">paper template</a>. 
                    Please use the template in traditional double-column format to prepare your submissions.
                    Please comment all the author information for submission and review of manuscript.

                    <br>
                    <br>

                    <b>Length:</b> The manuscript’s length is limited to one of the two options: 
                    <b>a) 4 pages plus 1-page reference; or b) 8 pages plus up to 2-page reference.</b>
                    The reference pages must only contain references. Overlength papers will be rejected without review.
                    We do not allow appendix that follow right after the main paper in the main submission file.

                    <br>
                    <br>

                    <b>Peer Review and publication in ACM Digital Library:</b> 
                    Paper submissions must conform with the “double-blind” review policy. 
                    All papers will be peer-reviewed by experts in the field, they will receive at least two reviews. 
                    Acceptance will be based on relevance to the workshop, scientific novelty, and technical quality.
                    The workshop papers will be published in the ACM Digital Library.

                    The Microsoft CMT service was used for managing the peer-reviewing process for this conference.
                        This service was provided for free by Microsoft and they bore all expenses, 
                        including costs for Azure cloud services as well as for software development and support.
                        
                    <br>

                    <b>Paper sumbission link</b>: <a href="https://cmt3.research.microsoft.com/MER2025"> https://cmt3.research.microsoft.com/MER2025

                    <br>

                    <!-- <b>Accepted Papers:</b> 
                    
                    <br>
                    <b> [Baseline] <a href="https://arxiv.org/abs/2404.17113">MER 2024: Semi-Supervised Learning, Noise Robustness, and Open-Vocabulary Multimodal Emotion Recognition.</a></b>
                    <br>
                    Zheng Lian, Haiyang Sun, Licai Sun, Zhuofan Wen, Siyuan Zhang, Shun Chen, Hao Gu, Jinming Zhao, Ziyang Ma, Xie Chen, Jiangyan Yi, Rui Liu, Kele Xu, Bin Liu, Erik Cambria, Guoying Zhao, Björn W. Schuller, Jianhua Tao
                    <br>
                    <br>

                    <b> [MER-SEMI] <a href="https://dl.acm.org/doi/abs/10.1145/3689092.3689401">Multimodal Emotion Recognition with Vision-language Prompting and Modality Dropout.</a></b>
                    <br>
                    Anbin Qi, Zhongliang Liu, Xinyong Zhou, Jinba Xiao, Fengrun Zhang, Qi Gan, Ming Tao, Gaozheng Zhang, Lu Zhang
                    <br>
                    <b> [MER-SEMI] <a href="https://dl.acm.org/doi/abs/10.1145/3689092.3689415">Early Joint Learning of Emotion Information Makes MultiModal Model Understand You Better.</a></b>
                    <br>
                    Mengying Ge, Mingyang Li, Dongkai Tang, Pengbo Li, Kuo Liu, Shuhao Deng, Songbai Pu, Long Liu, Yang Song, Tao Zhang
                    <br>
                    <b> [MER-SEMI] <a href="https://dl.acm.org/doi/abs/10.1145/3689092.3689414">Audio-Guided Fusion Techniques for Multimodal Emotion Analysis.</a></b>
                    <br>
                    Pujin Shi, Fei Gao
                    <br>
                    <b> [MER-SEMI] <a href="https://dl.acm.org/doi/abs/10.1145/3689092.3689407">Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment.</a></b>
                    <br>
                    Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, Lei Xie
                    <br>
                    <b> [MER-SEMI] <a href="https://dl.acm.org/doi/abs/10.1145/3689092.3689412">Leveraging Contrastive Learning and Self-Training for Multimodal Emotion Recognition with Limited Labeled Samples.</a></b>
                    <br>
                    Qi Fan, Yutong Li, Yi Xin, Xinyu Cheng, Guanglai Gao, Miao Ma
                    <br>
                    <br>

                    <b> [MER-NOISE] <a href="https://dl.acm.org/doi/abs/10.1145/3689092.3689404">SZTU-CMU at MER2024: Improving Emotion-LLaMA with Conv-Attention for Multimodal Emotion Recognition.</a>  </b>
                    <br>
                    Zebang Cheng, Shuyuan Tu, Dawei Huang, Minghan Li, Xiaojiang Peng, Zhi-Qi Cheng, Alexander G. Hauptmann
                    <br>
                    <b> [MER-NOISE] <a href="https://dl.acm.org/doi/abs/10.1145/3689092.3689415">Early Joint Learning of Emotion Information Makes MultiModal Model Understand You Better.</a></b>
                    <br>
                    Mengying Ge, Mingyang Li, Dongkai Tang, Pengbo Li, Kuo Liu, Shuhao Deng, Songbai Pu, Long Liu, Yang Song, Tao Zhang
                    <br>
                    <b> [MER-NOISE] <a href="https://dl.acm.org/doi/abs/10.1145/3689092.3689399">Multimodal Blockwise Transformer for Robust Sentiment Recognition.</a></b>
                    <br>
                    Zhengqin Lai, Xiaopeng Hong, Yabin Wang
                    <br>
                    <b> [MER-NOISE] <a href="https://dl.acm.org/doi/abs/10.1145/3689092.3689418">Robust Representation Learning for Multimodal Emotion Recognition with Contrastive Learning and Mixup.</a></b>
                    <br>
                    Yunrui Cai, Runchuan Ye, Jingran Xie, Yaoxun Xu, Yixuan Zhou, Zhiyong Wu
                    <br>
                    <br>

                    <b> [MER-OV] <a href="https://arxiv.org/pdf/2408.11286">Video Emotion Open-vocabulary Recognition Based on Multimodal Large Language Model.</a></b>
                    <br>
                    Mengying Ge, Dongkai Tang, Mingyang Li
                    <br>
                    <b> [MER-OV] <a href="https://dl.acm.org/doi/abs/10.1145/3689092.3689402">Open Vocabulary Emotion Prediction Based on Large Multimodal Models.</a></b>
                    <br>
                    Zixing Zhang, Zhongren Dong, Zhiqiang Gao, Shihao Gao, Donghao Wang, Ciqiang Chen, Yuhan Nie, Huan Zhao
                    <br>
                    <b> [MER-OV] <a href="https://dl.acm.org/doi/abs/10.1145/3689092.3689404">SZTU-CMU at MER2024: Improving Emotion-LLaMA with Conv-Attention for Multimodal Emotion Recognition.</a>  </b>
                    <br>
                    Zebang Cheng, Shuyuan Tu, Dawei Huang, Minghan Li, Xiaojiang Peng, Zhi-Qi Cheng, Alexander G. Hauptmann
                    <br>
                    <b> [MER-OV] <a href="https://dl.acm.org/doi/abs/10.1145/3689092.3689403">Multimodal Emotion Captioning Using Large Language Model with Prompt Engineering.</a></b>
                    <br>
                    Yaoxun Xu, Yixuan Zhou, Yunrui Cai, Jingran Xie, Runchuan Ye, Zhiyong Wu
                    <br>
                    <br>

                    <b> [Workshop] <a href="https://dl.acm.org/doi/abs/10.1145/3689092.3689408">MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Subtle Clue Dynamics in Video Dialogues.</a></b>
                    <br>
                    Liyun Zhang, Zhaojie Luo, Shuqiong Wu, Yuta Nakashima
                    <br>
                    <b> [Workshop] <a href="https://dl.acm.org/doi/abs/10.1145/3689092.3689411">Learning Noise-Robust Joint Representation for Multimodal Emotion Recognition under Incomplete Data Scenarios.</a></b>
                    <br>
                    Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian, Guanglai Gao
                    <br>
                    <br> -->
                </p>

                <p class="large text-muted">
                    <b>Contact email:</b>  merchallenge.contact@gmail.com; lianzheng2016@ia.ac.cn
                </p>
                

            </div>
        </div>
    </div>
</section>



 <!--Key Dates Section -->
 <section id="schedule">
    <div class="container">
        <div class="row">
            <div class="col-lg-12 text-center">
                <h2 class="section-heading">Schedule</h2>
                <!-- <h3 class="section-subheading text-muted">Lorem ipsum dolor sit amet consectetur.</h3--> 
            </div>
        </div>
        <div class="row">
            <div class="col-md-12 text-left">
                &nbsp;
            </div>
            <div class="text-left" style="text-align: center;">
                <div class="col-md-12">

                    <p class="large text-muted">
                        <del><span>Apr 30, 2025:</span> <b>Data, baseline paper & code available</b></del>
                    </p>
                    
                    <p class="large text-muted">
                        <del><span> Jun 26, 2025:</span> <b>Results submission start</b></del>
                    </p>

                    <p class="large text-muted">
                        <del><span> Jul 13, 2025:</span> <b>Results submission deadline</b></del>
                    </p>

                    <p class="large text-muted">
                        <del><span> Jul 22, 2025:</span> <b>Paper submission deadline</b></del>
                    </p>

                    <p class="large text-muted">
                        <del><span> Aug 1, 2025:</span> <b>Paper acceptance notification</b></del>
                    </p>

                    <p class="large text-muted">
                        <del><span> Aug 11, 2025:</span> <b>Deadline for camera-ready papers</b></del>
                    </p>

                    <p class="large text-muted">
                        <span> Oct 27-31, 2025:</span> <b>MRAC25 workshop@ACM MM (Dublin, Ireland)</b>
                    </p>

                    <br>

                    <p class="large text-muted">
                        <span> All submission deadlines are at 23:59 Anywhere on Earth (AoE).</b>
                    </p>
                    
                </div>
                <!-- <iframe src="img/schedule.pdf" width="600" height="500"></iframe> -->
                <!-- <embed src="img/schedule.pdf" width="1200" height="1000" type="application/pdf"> -->
                
                <!-- <div class="row text-justify">
                    <div class="col-md-12" >
        
                        <p class="large text-muted ">
                            <br>
                            Each speaker is required to attend in person, and each paper will be presented orally for a total of 10 minutes (8 minutes for the presentation and 2 minutes for questions).
                            <br>
                            <br>
                            <b> We share the <a href="https://pan.baidu.com/s/1VhM69CLjnxEDTQchyRbzQA?pwd=bqfc"> Recorded video </a> for our workshop</b>
                        </p>
                    </div>
                </div>
                <img src="img/schedule.png" alt="schedule" width="1160" height="800"> -->
            </div>
        </div>
    </div>
</section> 



    <!-- Speakers Section -->

    <section id="speakers"  class="bg-mid-gray" >
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">speakers</h2>
                </div>
            </div>
         
            
            <div class="row speakers">
                <div class="col-sm-4">
                    <div class="team-member">
                        <a href="https://mikecheninoulu.github.io/" target="_blank">
                            <img src="./img/qinjin.jpg" style="height: 200px; width:200px;" class="img-responsive img-circle">
                        </a>
                        <h4><a href="https://www.jin-qin.com/index.html" target="_blank">Qin Jin (TBD)</a></h4>
                        <p class="text-muted">Professor <br> Renmin University of China </p>

                    </div>
                </div>

                <!-- <div class="col-sm-8 text-justify ">
                    <h3>Title: Human emotion understanding via human behaviors and go beyond
                    </h3>
                    <p class="text-muted" style="font-size: 16px;">
                        Understanding human emotions has long relied on static expressions, explicit annotations, and language-centric labels. But human feelings are often embodied—hidden in micro-gestures, spontaneous actions, and unspoken behavioral cues.

In this talk, I will present a behavior-centric datasets, models and paradigms for emotion understanding, which leverages a non verbal communicative signal— micro gesture movement — to infer emotions beyond verbal self-report. I will highlight recent advances in modeling spontaneous human behavior, and our attempts to go beyond traditional emotion classification.

Moreover, I will discuss how these efforts lead to a deeper form of affective computing—one that is grounded in cognition, context, and even unconscious cues—paving the way for truly sentient and human-aligned AI systems.
                    </p>
                </div> -->
            </div>

            <div class="row speakers">
                <div class="col-sm-4">
                    <div class="team-member">
                        <a href="https://mikecheninoulu.github.io/" target="_blank">
                            <img src="./img/haoyu.png" style="height: 200px; width:200px;" class="img-responsive img-circle">
                        </a>
                        <h4><a href="https://mikecheninoulu.github.io/" target="_blank">Haoyu Chen</a></h4>
                        <p class="text-muted">Assistant Professor <br> University of Oulu </p>

                    </div>
                </div>

                <div class="col-sm-8 text-justify ">
                    <h3>Title: Human emotion understanding via human behaviors and go beyond
                    </h3>
                    <p class="text-muted" style="font-size: 16px;">
                        Understanding human emotions has long relied on static expressions, explicit annotations, and language-centric labels. But human feelings are often embodied—hidden in micro-gestures, spontaneous actions, and unspoken behavioral cues.

In this talk, I will present a behavior-centric datasets, models and paradigms for emotion understanding, which leverages a non verbal communicative signal— micro gesture movement — to infer emotions beyond verbal self-report. I will highlight recent advances in modeling spontaneous human behavior, and our attempts to go beyond traditional emotion classification.

Moreover, I will discuss how these efforts lead to a deeper form of affective computing—one that is grounded in cognition, context, and even unconscious cues—paving the way for truly sentient and human-aligned AI systems.
                    </p>
                </div>
            </div>


    </section>






    <!-- Organization Section -->
    <section id="organization">

            <div class="row">
                <div class="col-lg-12 text-center" style="padding-bottom: 20px;">
                    <h2 class="section-heading">ORGANISERS</h2>
                </div>
            </div>

            <div class="container">
                <div class="row">
                    <div class="col-sm-1">
                        &nbsp;
                    </div>

                    <div class="col-sm-2">
                        <div class="team-member">
                            <img src="./img/jhtao.png" class="img-responsive img-circle" alt="" width="200" height="300">
                            <h4><a href="https://www.au.tsinghua.edu.cn/info/1080/3219.htm">Jianhua Tao</a></h4>
                            <p class="text-muted"> Tsinghua University </p>
                        </div>
                    </div>

                    <div class="col-sm-2">
                        &nbsp;
                    </div>
					
                    <div class="col-sm-2">
                        <div class="team-member">
                            <img src="./img/lianzheng.jpg" class="img-responsive img-circle" alt="" width="200" height="300">
                            <h4><a href="https://zeroqiaoba.github.io/Homepage/">Zheng Lian</a></h4>
                            <p class="text-muted"> Institute of Automation, Chinese Academy of Sciences</p>
                        </div>
                    </div>

                    <div class="col-sm-2">
                        &nbsp;
                    </div>

                    <div class="col-sm-2">
                        <div class="team-member">
                            <img src="./img/Björn W. Schuller.jpg" class="img-responsive img-circle" alt="" width="200" height="300">
                            <h4><a href="http://www.schuller.one/">Björn W. Schuller</a></h4>
                            <p class="text-muted"> Technical University of Munich & Imperial College London </p>
                        </div>
                    </div>

                  </div>

              </div>

                
            <div class="container">
              <div class="row">
                <div class="col-sm-1">
                    &nbsp;
                </div>
                <div class="col-sm-2">
                    &nbsp;
                </div>
               

                <div class="col-sm-2">
                  <div class="team-member">
                      <img src="./img/Guoying_2023.jpg" class="img-responsive img-circle" alt="" width="200" height="300">
                      <h4><a href="https://gyzhao-nm.github.io/Guoying/"> Guoying Zhao </a></h4>
                      <p class="text-muted">  University of Oulu </p>
                  </div>
                </div>

                <div class="col-sm-2">
                    &nbsp;
                </div>

                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="./img/Erik Cambria.jpg" class="img-responsive img-circle" alt="" width="200" height="300">
                        <h4><a href="https://sentic.net/team/"> Erik Cambria </a></h4>
                        <p class="text-muted"> Nanyang Technological University </p>
                    </div>
                </div>

                <div class="col-sm-2">
                    &nbsp;
                </div>

              </div>
            </div>

        </div>
		
		    
        <div class="row" >
            <div class="col-lg-12 text-center" style="padding-bottom: 20px;">
                <h2 class="section-heading">Challenge Chairs</h2>
            </div>
        </div>

        <div class="container">
            <div class="row">
                <div class="col-sm-1">
                    &nbsp;
                </div>
                
                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="./img/liurui.jpg" class="img-responsive img-circle" alt="" width="200" height="300">
                        <h4><a href="https://ttslr.github.io/people.html">Rui Liu</a></h4>
                        <p class="text-muted"> Inner Mongolia University </p>
                    </div>
                </div>
                
                
				
				<div class="col-sm-2">
                    &nbsp;
                </div>
				
                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="./img/xukele.png" class="img-responsive img-circle" alt="" width="200" height="300">
                        <h4><a href="https://kelelexu.github.io/">Kele Xu</a></h4>
                        <p class="text-muted"> National University of Defense Technology</p>
                    </div>
                </div>
                
				<div class="col-sm-2">
                    &nbsp;
                </div>
				
                

                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="./img/liubin.jpg" class="img-responsive img-circle" alt="" width="200" height="300">
                        <h4><a href="https://people.ucas.edu.cn/~bin.liu">Bin Liu</a></h4>
                        <p class="text-muted"> Institute of Automation, Chinese Academy of Sciences </p>
                    </div>
                </div>

                
                
              </div>

          </div>
        </div>


        <div class="container">
            <div class="row">
                <div class="col-sm-1">
                    &nbsp;
                </div>
                
                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="./img/xuefei.jpg" class="img-responsive img-circle" alt="" width="200" height="300">
                        <h4><a href=""> Xuefei Liu </a></h4>
                        <p class="text-muted"> Tianjin Normal University </p>
                    </div>
                </div>

				
				<div class="col-sm-2">
                    &nbsp;
                </div>
				
                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="./img/liya.jpg" class="img-responsive img-circle" alt="" width="200" height="300">
                        <h4><a href="https://scholar.google.com/citations?user=ISK42qAAAAAJ&hl=en&oi=ao">Ya Li</a></h4>
                        <p class="text-muted"> Beijing University of Posts and Telecommunications </p>
                    </div>
                </div>

                
				<div class="col-sm-2">
                    &nbsp;
                </div>
				
                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="./img/jinming.jpg" class="img-responsive img-circle" alt="" width="200" height="300">
                        <h4><a href="https://scholar.google.com/citations?user=XskCnD8AAAAJ&hl=en&oi=ao"> Jinming Zhao </a></h4>
                        <p class="text-muted"> Qiyuan Lab </p>
                    </div>
                </div>
                
                
              </div>

          </div>
        </div>
        


        <div class="row" >
            <div class="col-lg-12 text-center" style="padding-bottom: 20px;">
                <h2 class="section-heading">Workshop Chairs</h2>
            </div>
        </div>

        <div class="container">
            <div class="row">
                <div class="col-sm-1">
                    &nbsp;
                </div>
                
                
                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="./img/yazhou.png" class="img-responsive img-circle" alt="" width="200" height="300">
                        <h4><a href="https://yzzhang2008.github.io/">Yazhou Zhang</a></h4>
                        <p class="text-muted"> Tianjin University </p>
                    </div>
                </div>

				<div class="col-sm-2">
                    &nbsp;
                </div>
				
                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="./img/xinliu.png" class="img-responsive img-circle" alt="" width="200" height="300">
                        <h4><a href="https://sites.google.com/view/xinliu/">Xin Liu</a></h4>
                        <p class="text-muted"> Lappeenranta-Lahti University of Technology </p>
                    </div>
                </div>
				
				<div class="col-sm-2">
                    &nbsp;
                </div>
				
                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="./img/xiaojiang.jpg" class="img-responsive img-circle" alt="" width="200" height="300">
                        <h4><a href="https://pengxj.github.io/">Xiaojiang Peng</a></h4>
                        <p class="text-muted"> Shenzhen Technology University </p>
                    </div>
                </div>
                
              </div>

          </div>
        </div>


        <div class="container">
            <div class="row">
                <div class="col-sm-1">
                    &nbsp;
                </div>
                
                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="./img/yongli.png" class="img-responsive img-circle" alt="" width="200" height="300">
                        <h4><a href="https://mysee1989.github.io/english">Yong Li</a></h4>
                        <p class="text-muted"> Southeast University </p>
                    </div>
                </div>
               

				<div class="col-sm-2">
                    &nbsp;
                </div>
				
                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="./img/xiechen.jpeg" class="img-responsive img-circle" alt="" width="200" height="300">
                        <h4><a href="https://chenxie95.github.io/">Xie Chen</a></h4>
                        <p class="text-muted"> Shanghai Jiao Tong University </p>
                    </div>
                </div>
                
                <div class="col-sm-2">
                    &nbsp;
                </div>

                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="./img/lica.jpg" class="img-responsive img-circle" alt="" width="200" height="300">
                        <h4><a href="https://sunlicai.github.io/">Licai Sun</a></h4>
                        <p class="text-muted"> University of Oulu </p>
                    </div>
                </div>
                
              </div>

          </div>
        </div>


<div class="row" >
    <div class="col-lg-12 text-center" style="padding-bottom: 20px;">
        <h2 class="section-heading">Data Chairs</h2>
    </div>
</div>

<div class="container">
    <div class="row">
        <div class="col-sm-1">
            &nbsp;
        </div>
        
        
        <div class="col-sm-2">
            <div class="team-member">
                <img src="./img/zebang.jpg" class="img-responsive img-circle" alt="" width="200" height="300">
                <h4><a href="https://scholar.google.com/citations?user=-fG3MhYAAAAJ&hl=en&oi=ao">Zebang Cheng</a></h4>
                <p class="text-muted"> Shenzhen University </p>
            </div>
        </div>

        <div class="col-sm-2">
            &nbsp;
        </div>
        
        <div class="col-sm-2">
            <div class="team-member">
                <img src="./img/haolin.jpg" class="img-responsive img-circle" alt="" width="200" height="300">
                <h4><a href="https://scholar.google.com/citations?hl=en&user=j45wL-kAAAAJ">Haolin Zuo</a></h4>
                <p class="text-muted"> Inner Mongolia University </p>
            </div>
        </div>
        
        <div class="col-sm-2">
            &nbsp;
        </div>

        <div class="col-sm-2">
            <div class="team-member">
                <img src="./img/ziyang.png" class="img-responsive img-circle" alt="" width="200" height="300">
                <h4><a href="https://ziyang.tech/">Ziyang Ma</a></h4>
                <p class="text-muted"> Shanghai Jiao Tong University </p>
            </div>
        </div>

        <div class="col-sm-2">
            &nbsp;
        </div>
        
      </div>

  </div>
</div>



        

        

    </section>
</section>



    <!-- jQuery -->
    <script src="./js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="./js/bootstrap.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="./js/jquery.easing.min.js"></script>
    <script src="./js/classie.js"></script>
    <script src="./js/cbpAnimatedHeader.js"></script>

    <!-- Contact Form JavaScript -->
    <script src="./js/contact_me.js"></script>

    <!-- Custom Theme JavaScript -->
    <script src="./js/agency.js"></script>



 <!--Key Dates Section -->
 <!-- <section id="WeChat Group"  class="bg-mid-gray" >
    <div class="container">
        <div class="row">
            <div class="col-lg-12 text-center">
                <h2 class="section-heading">WeChat Group</h2>
            </div>
        </div>
        <div class="row">
            <div class="col-md-12 text-left">
                &nbsp;
            </div>
            <div class="text-left" style="text-align: center;">
                <embed src="img/wechat.jpg" width="300" height="400">
            </div>
        </div>
    </div>
</section>  -->

</body></html>
